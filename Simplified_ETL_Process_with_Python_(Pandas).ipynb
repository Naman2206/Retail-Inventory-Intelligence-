{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Naman2206/Retail-Inventory-Intelligence-/blob/main/Simplified_ETL_Process_with_Python_(Pandas).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# --- Configuration ---\n",
        "# Define the base directory for raw and curated data\n",
        "BASE_DIR = \"data_engineering_project\"\n",
        "RAW_DATA_DIR = os.path.join(BASE_DIR, \"raw_data\")\n",
        "CURATED_DATA_DIR = os.path.join(BASE_DIR, \"curated_data\")\n",
        "\n",
        "# Ensure directories exist\n",
        "os.makedirs(RAW_DATA_DIR, exist_ok=True)\n",
        "os.makedirs(CURATED_DATA_DIR, exist_ok=True)\n",
        "\n",
        "# --- 1. Simulate Large Dataset Generation (Extract - Simplified) ---\n",
        "# In a real scenario, this would be reading from S3, Kafka, DB, etc.\n",
        "# We'll simulate daily transaction data for a few days.\n",
        "\n",
        "print(\"--- Simulating Raw Data Generation ---\")\n",
        "num_days = 3\n",
        "start_date = datetime(2025, 7, 23)\n",
        "\n",
        "for i in range(num_days):\n",
        "    current_date = start_date + timedelta(days=i)\n",
        "    date_str = current_date.strftime(\"%Y-%m-%d\")\n",
        "    file_path = os.path.join(RAW_DATA_DIR, f\"transactions_{date_str}.csv\")\n",
        "\n",
        "    # Simulate transaction data for the day\n",
        "    data = {\n",
        "        'transaction_id': range(100 * i, 100 * i + 50), # 50 transactions per day\n",
        "        'customer_id': [f'CUST_{j % 10}' for j in range(50)],\n",
        "        'product_id': [f'PROD_{j % 5}' for j in range(50)],\n",
        "        'quantity': [j % 5 + 1 for j in range(50)],\n",
        "        'price': [round((j % 10 + 1) * 10.5, 2) for j in range(50)],\n",
        "        'timestamp': [(current_date + timedelta(hours=j)).isoformat() for j in range(50)],\n",
        "        'status': ['completed' if j % 7 != 0 else 'failed' for j in range(50)]\n",
        "    }\n",
        "    df_raw = pd.DataFrame(data)\n",
        "\n",
        "    # Save raw data (simulating ingestion into a raw layer)\n",
        "    df_raw.to_csv(file_path, index=False)\n",
        "    print(f\"Generated raw data for {date_str} at: {file_path}\")\n",
        "\n",
        "# --- 2. ETL Process (Per-Day Partitioning & Transformation) ---\n",
        "\n",
        "print(\"\\n--- Starting ETL Process ---\")\n",
        "\n",
        "# Simulate a product lookup table for enrichment\n",
        "products_lookup = pd.DataFrame({\n",
        "    'product_id': [f'PROD_{j}' for j in range(5)],\n",
        "    'product_name': [f'Product {j+1}' for j in range(5)],\n",
        "    'product_category': [['Electronics', 'Home Goods', 'Apparel', 'Books', 'Food'][j % 5] for j in range(5)]\n",
        "})\n",
        "\n",
        "for i in range(num_days):\n",
        "    current_date = start_date + timedelta(days=i)\n",
        "    date_str = current_date.strftime(\"%Y-%m-%d\")\n",
        "    raw_file_path = os.path.join(RAW_DATA_DIR, f\"transactions_{date_str}.csv\")\n",
        "\n",
        "    if not os.path.exists(raw_file_path):\n",
        "        print(f\"Raw file not found for {date_str}. Skipping ETL for this day.\")\n",
        "        continue\n",
        "\n",
        "    print(f\"\\nProcessing data for {date_str}...\")\n",
        "\n",
        "    # --- Extract ---\n",
        "    # Read the raw daily transaction file\n",
        "    try:\n",
        "        df_transactions = pd.read_csv(raw_file_path)\n",
        "        print(f\"Extracted {len(df_transactions)} records from {raw_file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading {raw_file_path}: {e}\")\n",
        "        continue\n",
        "\n",
        "    # --- Transform ---\n",
        "    # 2.1. Data Cleaning: Handle missing values (though our simulated data is clean)\n",
        "    # For demonstration, let's assume 'quantity' could be missing and fill with 1\n",
        "    df_transactions['quantity'] = df_transactions['quantity'].fillna(1)\n",
        "    # Convert timestamp to datetime objects\n",
        "    df_transactions['timestamp'] = pd.to_datetime(df_transactions['timestamp'])\n",
        "\n",
        "    # 2.2. Data Enrichment: Join with product lookup table\n",
        "    df_transactions = pd.merge(df_transactions, products_lookup, on='product_id', how='left')\n",
        "    # Handle cases where product_id might not be in lookup\n",
        "    df_transactions['product_name'] = df_transactions['product_name'].fillna('Unknown Product')\n",
        "    df_transactions['product_category'] = df_transactions['product_category'].fillna('Unknown Category')\n",
        "\n",
        "    # 2.3. Data Aggregation / New Feature Creation: Calculate total amount\n",
        "    df_transactions['total_amount'] = df_transactions['quantity'] * df_transactions['price']\n",
        "\n",
        "    # 2.4. Filtering: Only keep 'completed' transactions for curated data\n",
        "    df_curated = df_transactions[df_transactions['status'] == 'completed'].copy()\n",
        "\n",
        "    # Select and reorder columns for the curated dataset\n",
        "    df_curated = df_curated[[\n",
        "        'transaction_id', 'customer_id', 'product_id', 'product_name',\n",
        "        'product_category', 'quantity', 'price', 'total_amount', 'timestamp'\n",
        "    ]]\n",
        "\n",
        "    print(f\"Transformed data: {len(df_curated)} completed transactions.\")\n",
        "\n",
        "    # --- Load ---\n",
        "    # Create partitioned directory for curated data (e.g., curated_data/year=2025/month=07/day=23/)\n",
        "    curated_partition_dir = os.path.join(\n",
        "        CURATED_DATA_DIR,\n",
        "        f\"year={current_date.year}\",\n",
        "        f\"month={current_date.month:02d}\",\n",
        "        f\"day={current_date.day:02d}\"\n",
        "    )\n",
        "    os.makedirs(curated_partition_dir, exist_ok=True)\n",
        "\n",
        "    curated_file_path = os.path.join(curated_partition_dir, f\"transactions_curated_{date_str}.parquet\")\n",
        "\n",
        "    # Save the curated data in Parquet format (efficient for analytical queries)\n",
        "    # In a real scenario, this would be loading to BigQuery, Snowflake, etc.\n",
        "    df_curated.to_parquet(curated_file_path, index=False)\n",
        "    print(f\"Loaded curated data to: {curated_file_path}\")\n",
        "\n",
        "print(\"\\n--- ETL Process Completed ---\")\n",
        "\n",
        "# --- 3. Verification (Optional) ---\n",
        "print(\"\\n--- Verifying Curated Data ---\")\n",
        "# Let's read one of the curated files to show it worked\n",
        "example_date = start_date + timedelta(days=num_days - 1)\n",
        "example_date_str = example_date.strftime(\"%Y-%m-%d\")\n",
        "\n",
        "example_curated_path = os.path.join(\n",
        "    CURATED_DATA_DIR,\n",
        "    f\"year={example_date.year}\",\n",
        "    f\"month={example_date.month:02d}\",\n",
        "    f\"day={example_date.day:02d}\",\n",
        "    f\"transactions_curated_{example_date_str}.parquet\"\n",
        ")\n",
        "\n",
        "if os.path.exists(example_curated_path):\n",
        "    df_verified = pd.read_parquet(example_curated_path)\n",
        "    print(f\"\\nSuccessfully read {len(df_verified)} records from curated data for {example_date_str}:\")\n",
        "    print(df_verified.head())\n",
        "else:\n",
        "    print(f\"Could not find example curated file at: {example_curated_path}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Simulating Raw Data Generation ---\n",
            "Generated raw data for 2025-07-23 at: data_engineering_project/raw_data/transactions_2025-07-23.csv\n",
            "Generated raw data for 2025-07-24 at: data_engineering_project/raw_data/transactions_2025-07-24.csv\n",
            "Generated raw data for 2025-07-25 at: data_engineering_project/raw_data/transactions_2025-07-25.csv\n",
            "\n",
            "--- Starting ETL Process ---\n",
            "\n",
            "Processing data for 2025-07-23...\n",
            "Extracted 50 records from data_engineering_project/raw_data/transactions_2025-07-23.csv\n",
            "Transformed data: 42 completed transactions.\n",
            "Loaded curated data to: data_engineering_project/curated_data/year=2025/month=07/day=23/transactions_curated_2025-07-23.parquet\n",
            "\n",
            "Processing data for 2025-07-24...\n",
            "Extracted 50 records from data_engineering_project/raw_data/transactions_2025-07-24.csv\n",
            "Transformed data: 42 completed transactions.\n",
            "Loaded curated data to: data_engineering_project/curated_data/year=2025/month=07/day=24/transactions_curated_2025-07-24.parquet\n",
            "\n",
            "Processing data for 2025-07-25...\n",
            "Extracted 50 records from data_engineering_project/raw_data/transactions_2025-07-25.csv\n",
            "Transformed data: 42 completed transactions.\n",
            "Loaded curated data to: data_engineering_project/curated_data/year=2025/month=07/day=25/transactions_curated_2025-07-25.parquet\n",
            "\n",
            "--- ETL Process Completed ---\n",
            "\n",
            "--- Verifying Curated Data ---\n",
            "\n",
            "Successfully read 42 records from curated data for 2025-07-25:\n",
            "   transaction_id customer_id product_id product_name product_category  \\\n",
            "0             201      CUST_1     PROD_1    Product 2       Home Goods   \n",
            "1             202      CUST_2     PROD_2    Product 3          Apparel   \n",
            "2             203      CUST_3     PROD_3    Product 4            Books   \n",
            "3             204      CUST_4     PROD_4    Product 5             Food   \n",
            "4             205      CUST_5     PROD_0    Product 1      Electronics   \n",
            "\n",
            "   quantity  price  total_amount           timestamp  \n",
            "0         2   21.0          42.0 2025-07-25 01:00:00  \n",
            "1         3   31.5          94.5 2025-07-25 02:00:00  \n",
            "2         4   42.0         168.0 2025-07-25 03:00:00  \n",
            "3         5   52.5         262.5 2025-07-25 04:00:00  \n",
            "4         1   63.0          63.0 2025-07-25 05:00:00  \n"
          ]
        }
      ],
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLBIPk_1AWJB",
        "outputId": "14a37a70-1a15-4cbd-849d-62aaf600ad45"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Owrg8kT9EMD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YYi4GIDDET9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New section"
      ],
      "metadata": {
        "id": "GQCwHRv5EnEs"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0iv8JIJiEthN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e_lJRnpsB2b9"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}